{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import argv\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkSession.builder.appName(\"LogisticRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/USCensus1990.data.txt\"\n",
    "data_raw = sc.read.csv(input_file, inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many rows in original data:  2458285\n"
     ]
    }
   ],
   "source": [
    "print(\"how many rows in original data: \", data_raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caseid: integer (nullable = true)\n",
      " |-- dAge: integer (nullable = true)\n",
      " |-- dAncstry1: integer (nullable = true)\n",
      " |-- dAncstry2: integer (nullable = true)\n",
      " |-- iAvail: integer (nullable = true)\n",
      " |-- iCitizen: integer (nullable = true)\n",
      " |-- iClass: integer (nullable = true)\n",
      " |-- dDepart: integer (nullable = true)\n",
      " |-- iDisabl1: integer (nullable = true)\n",
      " |-- iDisabl2: integer (nullable = true)\n",
      " |-- iEnglish: integer (nullable = true)\n",
      " |-- iFeb55: integer (nullable = true)\n",
      " |-- iFertil: integer (nullable = true)\n",
      " |-- dHispanic: integer (nullable = true)\n",
      " |-- dHour89: integer (nullable = true)\n",
      " |-- dHours: integer (nullable = true)\n",
      " |-- iImmigr: integer (nullable = true)\n",
      " |-- dIncome1: integer (nullable = true)\n",
      " |-- dIncome2: integer (nullable = true)\n",
      " |-- dIncome3: integer (nullable = true)\n",
      " |-- dIncome4: integer (nullable = true)\n",
      " |-- dIncome5: integer (nullable = true)\n",
      " |-- dIncome6: integer (nullable = true)\n",
      " |-- dIncome7: integer (nullable = true)\n",
      " |-- dIncome8: integer (nullable = true)\n",
      " |-- dIndustry: integer (nullable = true)\n",
      " |-- iKorean: integer (nullable = true)\n",
      " |-- iLang1: integer (nullable = true)\n",
      " |-- iLooking: integer (nullable = true)\n",
      " |-- iMarital: integer (nullable = true)\n",
      " |-- iMay75880: integer (nullable = true)\n",
      " |-- iMeans: integer (nullable = true)\n",
      " |-- iMilitary: integer (nullable = true)\n",
      " |-- iMobility: integer (nullable = true)\n",
      " |-- iMobillim: integer (nullable = true)\n",
      " |-- dOccup: integer (nullable = true)\n",
      " |-- iOthrserv: integer (nullable = true)\n",
      " |-- iPerscare: integer (nullable = true)\n",
      " |-- dPOB: integer (nullable = true)\n",
      " |-- dPoverty: integer (nullable = true)\n",
      " |-- dPwgt1: integer (nullable = true)\n",
      " |-- iRagechld: integer (nullable = true)\n",
      " |-- dRearning: integer (nullable = true)\n",
      " |-- iRelat1: integer (nullable = true)\n",
      " |-- iRelat2: integer (nullable = true)\n",
      " |-- iRemplpar: integer (nullable = true)\n",
      " |-- iRiders: integer (nullable = true)\n",
      " |-- iRlabor: integer (nullable = true)\n",
      " |-- iRownchld: integer (nullable = true)\n",
      " |-- dRpincome: integer (nullable = true)\n",
      " |-- iRPOB: integer (nullable = true)\n",
      " |-- iRrelchld: integer (nullable = true)\n",
      " |-- iRspouse: integer (nullable = true)\n",
      " |-- iRvetserv: integer (nullable = true)\n",
      " |-- iSchool: integer (nullable = true)\n",
      " |-- iSept80: integer (nullable = true)\n",
      " |-- iSex: integer (nullable = true)\n",
      " |-- iSubfam1: integer (nullable = true)\n",
      " |-- iSubfam2: integer (nullable = true)\n",
      " |-- iTmpabsnt: integer (nullable = true)\n",
      " |-- dTravtime: integer (nullable = true)\n",
      " |-- iVietnam: integer (nullable = true)\n",
      " |-- dWeek89: integer (nullable = true)\n",
      " |-- iWork89: integer (nullable = true)\n",
      " |-- iWorklwk: integer (nullable = true)\n",
      " |-- iWWII: integer (nullable = true)\n",
      " |-- iYearsch: integer (nullable = true)\n",
      " |-- iYearwrk: integer (nullable = true)\n",
      " |-- dYrsserv: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_raw.filter(data_raw.iSex.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "cols = data_df.columns\n",
    "excluded = set(['iSex', 'caseid'])\n",
    "vec_cols = [s for s in cols if s not in excluded]\n",
    "assembler = VectorAssembler(inputCols = vec_cols, outputCol = \"features\")\n",
    "\n",
    "data_df = assembler.transform(data_df)\n",
    "data_df = data_df.select(\"features\", 'iSex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|iSex|\n",
      "+--------------------+----+\n",
      "|(67,[0,2,5,6,7,8,...|   1|\n",
      "+--------------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform our dataset using VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "data_df = data_df.withColumn(\"label\", data_df[\"iSex\"].cast(IntegerType()))\n",
    "data_df = data_df.select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "\n",
    "## create a dataframe for w, otherwise no way to compute gradient\n",
    "v = [tuple(0.0 for i in range(len(vec_cols)))]\n",
    "w_cols = list(['v'+str(i) for i in range(len(vec_cols))])\n",
    "df_w = sc.createDataFrame(v, w_cols)\n",
    "vecAssembler = VectorAssembler(inputCols=w_cols, outputCol=\"w_vec\")\n",
    "df_w = vecAssembler.transform(df_w)\n",
    "df_w = df_w.select(\"w_vec\")\n",
    "w = Vectors.dense(df_w.take(1)[0][\"w_vec\"])\n",
    "\n",
    "def gradient(xi, yi):\n",
    "  dotprod = xi.dot(w)\n",
    "  prob = 1.0/(1.0 + np.exp(-1.0*dotprod*yi))\n",
    "  return Vector.dense((prob - 1.0)*yi*xi)\n",
    "dot_prod_udf = F.udf(lambda x,y: gradient(x,y), VectorUDT())\n",
    "\n",
    "data_prod = data_df.withColumn('dot_prod', dot_prod_udf(F.array('features'), F.col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+\n",
      "|            features|label|      feature_vector|     scaled_features|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "|(67,[0,2,5,6,7,8,...|    1|[5.0,0.0,1.0,0.0,...|[2.44082028870012...|\n",
      "|(67,[0,1,2,5,6,7,...|    1|[6.0,1.0,1.0,0.0,...|[2.92898434644015...|\n",
      "|(67,[0,1,2,5,6,7,...|    1|[3.0,1.0,2.0,0.0,...|[1.46449217322007...|\n",
      "|(67,[0,1,2,5,6,7,...|    1|[4.0,1.0,2.0,0.0,...|[1.95265623096010...|\n",
      "|(67,[0,1,2,7,8,11...|    1|[7.0,1.0,1.0,0.0,...|[3.41714840418018...|\n",
      "+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler as scaler\n",
    "\n",
    "Densify = F.udf(lambda s: DenseVector(s.toArray()), VectorUDT()) \n",
    "data_dense = data_df.withColumn('feature_vector', Densify(F.col('features')))\n",
    "\n",
    "standardscaler = scaler().setInputCol(\"feature_vector\").setOutputCol(\"scaled_features\")\n",
    "data = standardscaler.fit(data_dense).transform(data_dense)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select(\"scaled_features\", \"label\")\n",
    "\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1966628 data points with Spark Logistic Regression takes: 191.67984890937805 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"scaled_features\", maxIter=100)\n",
    "model = lr.fit(train_data)\n",
    "predict_data = model.transform(test_data)\n",
    "print(f\"Training on {int(2458285*0.8)} data points with Spark Logistic Regression takes: {float(time.time()-start)} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_data.select(\"label\",\"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+\n",
      "|     scaled_features|label|prediction|         probability|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "|[0.0,0.0,0.589570...|    0|       0.0|[0.99999999999993...|\n",
      "|[0.0,0.0,0.589570...|    0|       0.0|[0.99999999999993...|\n",
      "|[0.0,0.0,0.589570...|    0|       0.0|[0.99999999999993...|\n",
      "|[0.0,0.0,0.589570...|    0|       0.0|[0.99999999999190...|\n",
      "|[0.0,0.0,0.589570...|    0|       0.0|[0.99999999999190...|\n",
      "+--------------------+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The area under ROC for train set is 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\")\n",
    "predict_data.select(\"scaled_features\",\"label\",\"prediction\",\"probability\").show(5)\n",
    "print(\"The area under ROC for train set is {}\".format(evaluator.evaluate(predict_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
